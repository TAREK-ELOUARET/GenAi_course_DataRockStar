{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e062d22",
   "metadata": {},
   "source": [
    "# Leçon 5 — Transfert de Style Neuronal (Neural Style Transfer)\n",
    "\n",
    "## Objectifs\n",
    "- Séparer contenu vs style via des features CNN (VGG19)\n",
    "- Minimiser α·ContentLoss + β·StyleLoss (Gram matrices)\n",
    "- Régler couches, poids et pré/post-traitements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NST minimaliste avec PyTorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "prep = transforms.Compose([\n",
    "    transforms.Resize(384),\n",
    "    transforms.CenterCrop(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x[:3, ...])\n",
    "])\n",
    "device = torch.device('cpu')\n",
    "\n",
    "def load_img(path):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return prep(img).unsqueeze(0).to(device)\n",
    "\n",
    "# Remplacez par vos chemins de fichiers\n",
    "# content = load_img('content.jpg')\n",
    "# style   = load_img('style.jpg')\n",
    "print('Placez vos images content.jpg et style.jpg dans le répertoire courant puis exécutez à nouveau.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5376c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une fois les images placées, décommentez le bloc ci-dessous et exécutez-le.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    b, c, h, w = tensor.size()\n",
    "    feat = tensor.view(c, h*w)\n",
    "    G = torch.mm(feat, feat.t())\n",
    "    return G / (c*h*w)\n",
    "\n",
    "# vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to('cpu').eval()\n",
    "# content_layers = ['21']\n",
    "# style_layers   = ['0','5','10','19','28']\n",
    "\n",
    "# def get_features(x):\n",
    "#     feats = {}\n",
    "#     for name, layer in vgg._modules.items():\n",
    "#         x = layer(x)\n",
    "#         if name in content_layers + style_layers:\n",
    "#             feats[name] = x\n",
    "#     return feats\n",
    "\n",
    "# content_feats = get_features(content)\n",
    "# style_feats   = get_features(style)\n",
    "# style_grams   = {l: gram_matrix(style_feats[l]) for l in style_layers}\n",
    "\n",
    "# target = content.clone().requires_grad_(True)\n",
    "# alpha, beta = 1.0, 1e4\n",
    "# opt = optim.LBFGS([target], max_iter=200)\n",
    "# iter_count = [0]\n",
    "# def closure():\n",
    "#     opt.zero_grad()\n",
    "#     feats_t = get_features(target)\n",
    "#     content_loss = nn.functional.mse_loss(feats_t[content_layers[0]], content_feats[content_layers[0]])\n",
    "#     style_loss = 0.0\n",
    "#     for l in style_layers:\n",
    "#         Gt = gram_matrix(feats_t[l])\n",
    "#         style_loss += nn.functional.mse_loss(Gt, style_grams[l])\n",
    "#     loss = alpha*content_loss + beta*style_loss\n",
    "#     loss.backward()\n",
    "#     iter_count[0] += 1\n",
    "#     if iter_count[0] % 50 == 0:\n",
    "#         print(f'it={iter_count[0]} | total={loss.item():.2f} | content={content_loss.item():.2f} | style={style_loss.item():.2f}')\n",
    "#     return loss\n",
    "# opt.step(closure)\n",
    "# from torchvision import transforms\n",
    "# to_pil = transforms.ToPILImage()\n",
    "# out_img = target.detach().clamp(0,1).squeeze(0)\n",
    "# to_pil(out_img).save('stylized.png')\n",
    "# print('Image stylisée enregistrée : stylized.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d93696",
   "metadata": {},
   "source": [
    "> Astuce : commencez avec β élevé pour forcer le style, puis ajustez α/β pour préserver plus de contenu."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}