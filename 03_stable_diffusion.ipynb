{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe422df7",
   "metadata": {},
   "source": [
    "# Leçon 3 — Générer des images avec la diffusion (Stable Diffusion)\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre le processus bruitage→débruitage\n",
    "- Rôle du VAE, UNet, encodeur de texte\n",
    "- Paramètres: prompt, guidance scale, steps, seed\n",
    "\n",
    "## Pipeline\n",
    "1. Encoder le prompt (texte → embeddings)\n",
    "2. Partir d'un bruit latent\n",
    "3. UNet prédit le bruit à enlever sur plusieurs pas\n",
    "4. Decoder via VAE → image finale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70938d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stable Diffusion (diffusers) : génération basique ---\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# 1) Charger le pipeline (peut être lent/VRAM importante). Sur CPU, attendez-vous à un temps long.\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    'runwayml/stable-diffusion-v1-5', torch_dtype=torch.float32\n",
    ")\n",
    "pipe = pipe.to('cpu')  # ou 'cuda' si GPU\n",
    "\n",
    "# 2) Définir un prompt descriptif\n",
    "prompt = 'Un paysage marin au coucher du soleil, style peinture impressionniste, couleurs vives'\n",
    "\n",
    "# 3) Générer une image\n",
    "image = pipe(prompt, guidance_scale=7.5, num_inference_steps=30).images[0]\n",
    "image.save('sortie_diffusion.png')\n",
    "print('Image enregistrée : sortie_diffusion.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced049b",
   "metadata": {},
   "source": [
    "> Conseil : Ajustez `guidance_scale` (4–9) et `num_inference_steps` (20–50) pour équilibrer qualité et temps."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}